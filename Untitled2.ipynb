{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CR1onb0UTy8W"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.cuda.current_device()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " #download pytorch cuda\n",
        "# https://pytorch.org/get-started/locally/\n",
        "# uncomment and run this command in Terminal to monitor download progress and debug any error"
      ],
      "metadata": {
        "id": "WEuIwF4_UEXW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!conda install pytorch torchvision torchaudio pytorch-cuda=11.7 -c pytorch -c nvidia"
      ],
      "metadata": {
        "id": "GcA4c0VIT5cF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.set_device(0)\n",
        "torch.cuda.is_available(),torch.cuda.get_device_name()"
      ],
      "metadata": {
        "id": "CgUeOqxIUE8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "import os\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:24\"\n",
        "torch.cuda.memory_summary(device=None, abbreviated=False)"
      ],
      "metadata": {
        "id": "K5LP0WpWUIAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#uncomment and run this command on your machine\n",
        "# make sure, you have a Git installed on your machine if not\n",
        "# for linux run\n",
        "# sudo apt install git-all\n",
        "\n",
        "for windows, download git from this link\n",
        "https://git-scm.com/download/win\n"
      ],
      "metadata": {
        "id": "wj0r4DMJUQwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "git clone https://github.com/facebookresearch/llama.git"
      ],
      "metadata": {
        "id": "Iw42AiOXUYd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## WINDOWS\n",
        "# bash ./download.sh\n",
        "# if this commands error out, download wget.exe from below link and copy wget.ex to C:\\amit.la\\llama\n",
        "# https://eternallybored.org/misc/wget/\n",
        "# make sure, you include << C:\\amit.la\\llama >> to windows environment path so that windows can find it"
      ],
      "metadata": {
        "id": "zsNSwgJtUwtf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -e .\n",
        "!python setup.py install"
      ],
      "metadata": {
        "id": "PrdX3F-nUxQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " only for windows\n",
        "# change line #62 on llama/generate.py\n",
        "# from\n",
        "# << torch.distributed.init_process_group(\"gloo|nccl\") >>\n",
        "# to\n",
        "# << torch.distributed.init_process_group(\"gloo|nccl\") >>"
      ],
      "metadata": {
        "id": "MocdXh3-U5eY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -e .\n",
        " !python setup.py install"
      ],
      "metadata": {
        "id": "AXX3ASPfWZWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "./example_text_completion.py\n",
        " ./example_chat_completion.py\n",
        "\n",
        " change prompts | dialogue\n",
        "   prompts = [\n",
        "         For these prompts, the expected answer is the natural continuation of the prompt\n",
        "        \"meaning of life is\",\n",
        "    ]"
      ],
      "metadata": {
        "id": "XjIAIYbrWfc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "you are ready to use llama\n",
        " !torchrun --nproc_per_node 1 example_text_completion.py --ckpt_dir llama-2-7b --tokenizer_path tokenizer.model --max_seq_len 512 --max_batch_size 1"
      ],
      "metadata": {
        "id": "ayf0EC5bWlzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# install https://github.com/krychu/llama instead of https://github.com/facebookresearch/llama\n",
        "\n",
        "# execute download.sh in a new terminal, provide META AI URL and download 7B model and model weights\n",
        "#   run the download.sh script in a terminal, passing the URL provided when prompted to start the download\n",
        "# create a new env\n",
        "\n",
        "# python3 -m venv env\n",
        "# source env/bin/activate\n",
        "\n",
        "# pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu #pour la version cpu\n",
        "# python3 -m pip install -e .\n",
        "\n",
        "\n",
        "# torchrun --nproc_per_node 1 example_text_completion.py --ckpt_dir llama-2-7b/ --tokenizer_path tokenizer.model --max_seq_len 128 --max_batch_size 1"
      ],
      "metadata": {
        "id": "zukt12V4Wr6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yOujJ22KWww_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}